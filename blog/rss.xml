<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/">
    <channel>
        <title>Cloud Carbon Footprint Blog</title>
        <link>https://cloud-carbon-footprint.github.io/blog</link>
        <description>Cloud Carbon Footprint Blog</description>
        <lastBuildDate>Thu, 07 Sep 2023 00:00:00 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[CCF Data Collection: On-Premise]]></title>
            <link>https://cloud-carbon-footprint.github.io/blog/on-prem-data-collection</link>
            <guid>https://cloud-carbon-footprint.github.io/blog/on-prem-data-collection</guid>
            <pubDate>Thu, 07 Sep 2023 00:00:00 GMT</pubDate>
            <description><![CDATA[Using enterprise tooling to collect and populate the CCF Data Model]]></description>
            <content:encoded><![CDATA[<h2 class="anchor anchorWithStickyNavbar_LWe7" id="forward">Forward<a href="#forward" class="hash-link" aria-label="Direct link to Forward" title="Direct link to Forward">​</a></h2><p>Embarking on the journey of collecting data for your on-premise resources can at first seem like an impossible task. What data do I actually need? Do you have the right tooling? What stakeholders are you going to need, and how much time is this going to take? Where do you store the data? Most importantly, how do you get the data once you identify the tooling available to you? We’ll cover all of that below, but first just know that despite your initial trepidation; endpoint data exists in countless tools. With a little creativity and knowhow, you should have no trouble identifying, retrieving, and storing endpoint data to be utilized within the CCF Dashboard..</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="getting-started-with-on-premise-data-collection">Getting Started With On-Premise Data Collection<a href="#getting-started-with-on-premise-data-collection" class="hash-link" aria-label="Direct link to Getting Started With On-Premise Data Collection" title="Direct link to Getting Started With On-Premise Data Collection">​</a></h2><h3 class="anchor anchorWithStickyNavbar_LWe7" id="the-on-premise-data-model">The On-premise data model<a href="#the-on-premise-data-model" class="hash-link" aria-label="Direct link to The On-premise data model" title="Direct link to The On-premise data model">​</a></h3><p>The CCF CLI takes in on-premise data using a predefined model. This model dictates the type, content, and format of the data needed in order to facilitate and proper and accurate data import and manipulation. You can view the current custom data model, as well as methodology write ups in the <a href="https://www.cloudcarbonfootprint.org/docs/on-premise/" target="_blank" rel="noopener noreferrer">On-Premise</a> parts of the CCF docs . The data model contains a lot of common fields such as CPU cores, memory, and where in the world a particular machine is located. We’ll cover these in more detail as we continue on, however just keep in mind that we need all of this data for each endpoint to provide accurate estimates.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="data-sources">Data Sources<a href="#data-sources" class="hash-link" aria-label="Direct link to Data Sources" title="Direct link to Data Sources">​</a></h3><p>Your organization will most likely contain a multitude of sources from which endpoint data can originate. From computer accounts in Active Directory to System information stored within tools such as your Antivirus or system patching tools; large swathes of data are stored. It’s in these places you’ll be looking for your target endpoints and pulling relevant data from these sources to populate the On-Premise data model.</p><p>Endpoint data can also come from some unlikely places. If you are finding yourself lost in your search for endpoint data, think of the following list of tooling to see if perhaps your IT department has one of these tools:</p><ul><li>Antivirus suites</li><li>Vulnerability management suites</li><li>CMDB applications (Inventory management)</li><li>Configuration management suites<ul><li>Puppet</li><li>Chef</li><li>SCCM</li><li>MEM</li></ul></li><li>System monitoring suites<ul><li>Nagios</li><li>Cacti</li><li>Grafana<ul><li>Node Exporter</li><li>Telegraf</li><li>Prometheus Databases</li></ul></li></ul></li></ul><p>The list goes on and on. There are countless tools currently available for organizations to perform a wide range of tasks related to endpoint management, monitoring, and protection. The wonderful thing about tooling such as this is that they also rely on large amounts of data related to the endpoint. Antivirus tools need to know if the machine has been online and communicating with the management console for updates and status information. Monitoring tools rely solely on doing exactly what their name suggests–monitoring endpoints. CMDB tools are directly responsible for endpoint and inventory management. Many IT departments consider CMDB tooling a source of endpoint and inventory truth. Data here is likely to be very accurate.</p><font size="1">NOTE: Keep in mind the permissions needed to access these data sources, and ensure you’re following your company's best practices on the retrieval and storage of this data. Where possible, always partner with the data source owner to ensure you’re not only getting the best data, but handling it in a manner that doesn’t expose your company to unnecessary risks or security incidents.</font><br><p>Data sources are the most important part of embarking on a journey of tracking and estimating the environmental impacts of your enterprise. As you begin, be creative and leverage your existing partnerships with various departments within your organization to find these unlikely sources of data. Continue to rely on the On-premise Data Model to match data sources with the required fields.</p><p>When choosing a data source, be mindful of the size of the dataset, and your ability to continually pull data out. Intermittent, or services with unreliable uptimes are not preferable sources. Data sources with large datasets that cannot be filtered or compressed may prove difficult or costly to ingest. Always try to choose data sources that have exposed API’s if possible. This will make automating and scheduling data collection far simpler in the long run. If API’s are simply not possible, strive instead for tooling that can generate scheduled reports containing the data you need. If possible opt for CSV, JSON, XML outputs that you can use easily with your data collection and storage processes. We’ll cover this in the section <em>Collecting on-premise data</em>.</p><p>Before we move on to the means and methods in which you might collect and store this data, bear in mind that some of the fields in the On-premise Data Model are to be collected over time. We will go into greater detail about these fields later on. Just understand that your chosen data source does not necessarily have to include this data. You may be able to generate the data with good automation, and scheduled data retrieval from other information provided by the data source.</p><ul><li>dailyUptime</li><li>weeklyUptime</li><li>monthlyUptime</li><li>annualUptime</li></ul><p>Once you have a source of data to utilize, you’ll be using that data to calculate the above fields.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="data-collection-and-storage-technologies">Data Collection and Storage Technologies<a href="#data-collection-and-storage-technologies" class="hash-link" aria-label="Direct link to Data Collection and Storage Technologies" title="Direct link to Data Collection and Storage Technologies">​</a></h3><p>For example’s sake, let’s say you have chosen your AntiVirus as your initial data source. This tool contains all the basic information needed for the On-premise Data Model, and it has a robust REST API that can be utilized to fetch the data. Where do we go from here? Let’s have a look at a basic workflow diagram.</p><p><img loading="lazy" alt="Workflow Diagram" src="/assets/images/ccf_on_prem_blog_img_1-89a595ff6d485f7f59893e93ad132121.png" width="2000" height="190" class="img_ev3q"></p><p>This is the simplest possible way to represent the actions to be taken when it comes to collecting and storing data. Although simple to visualize, the means in which you achieve this goal can grow from very simple to very complex depending on the amount of data you will have, the length of time you’ll be holding onto it, and any security or risk considerations being taken into account. As with data sources, there are countless tools and techniques one can use to perform these actions.</p><p>Because we’ll need to capture this data over time, choose a data collection technology that allows for accurate and timely scheduled runs. Being able to collect data, daily, or by hour will greatly improve the accuracy of your data.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="aws-glue-with-aws-s3-data-lake">AWS Glue with AWS S3 Data Lake<a href="#aws-glue-with-aws-s3-data-lake" class="hash-link" aria-label="Direct link to AWS Glue with AWS S3 Data Lake" title="Direct link to AWS Glue with AWS S3 Data Lake">​</a></h4><p>This is an excellent cloud native approach to capturing large to extremely large amounts of data easily. AWS Glue provides capabilities such as scheduled, and interval based run initialization and the S3 Datalake can handle extremely large amounts of data.</p><p>Additionally you can pair this with AWS functionality such as AWS Athena, RDS, and Lambda to make transforming, and retrieving the data simple, effective, and accurate.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="django-with-django-rest-framework-celery-beat-and-postgresql">Django with Django-Rest-Framework, Celery-Beat, and PostgreSQL<a href="#django-with-django-rest-framework-celery-beat-and-postgresql" class="hash-link" aria-label="Direct link to Django with Django-Rest-Framework, Celery-Beat, and PostgreSQL" title="Direct link to Django with Django-Rest-Framework, Celery-Beat, and PostgreSQL">​</a></h4><p>If you have a Python developer available and are interested in a cost effective, or free solution; open source may be your best choice. Django is an extremely robust web framework built on the “View-Model” strategy. It contains an impressive number of built in capabilities including “CRUD” functionality which makes creating, and using database models a breeze.</p><p>With the addition of 2 additional plugins it can provide all the data storage and retrieval functionality you would need. Django-Rest-Framework makes creating API’s within Django simple, thus aiding in retrieving your stored data. Celery-Beat is a database-backed task scheduler that can make scheduling accurate and timely data collection a breeze.</p><p>Django also works incredibly well within containerized environments such as Docker, and Kubernetes.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="on-premise-data---data-model">On-premise Data - Data Model<a href="#on-premise-data---data-model" class="hash-link" aria-label="Direct link to On-premise Data - Data Model" title="Direct link to On-premise Data - Data Model">​</a></h3><p>Much of the data you’ll be collecting is very straightforward. Machine name, CPU type and memory count may be readily available. Some of the data however requires special consideration. Let’s explore those briefly.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="special-data-model-fields">Special Data Model Fields<a href="#special-data-model-fields" class="hash-link" aria-label="Direct link to Special Data Model Fields" title="Direct link to Special Data Model Fields">​</a></h4><p>The below data points will require special care when collecting the data. Take care to ensure that you build these considerations into your data model to ensure an accurate data collection process.</p><ul><li>machineType:<ul><li>Different machine types will generate different power loads. The formula used in calculating power draw for different types of machines will vary based on the type. If your data source includes a type, you should ensure that it is formatted to be either “server”, “laptop”, or “desktop”. Being unable to provide this information will impact the accuracy of your power usage calculations.</li></ul></li><li>cpuUtilization:<ul><li>Your data collection tool may be unable to provide this information. If that is the case, your best option is to make a best case estimation.</li></ul></li><li>[daily,weekly,monthly,annual]<!-- -->Uptime:<ul><li>These 4 fields are the most important, and also require a good data collection process. Using your data sources, you’ll need to provide incremental totals for these fields using uptime data if available. Not being able to provide this data will lead to very inaccurate carbon impact estimations.</li></ul></li></ul><h2 class="anchor anchorWithStickyNavbar_LWe7" id="collecting-on-premise-data">Collecting On-premise Data<a href="#collecting-on-premise-data" class="hash-link" aria-label="Direct link to Collecting On-premise Data" title="Direct link to Collecting On-premise Data">​</a></h2><p>Once you’ve identified a suitable data source and method, you can now begin the process of collecting and storing your on-premise data. For the purposes of the write-up we’re going to use an “Antivirus” as our data source.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="the-data-source---antivirus">The Data Source - Antivirus<a href="#the-data-source---antivirus" class="hash-link" aria-label="Direct link to The Data Source - Antivirus" title="Direct link to The Data Source - Antivirus">​</a></h3><p>Our data source is an Antivirus suite that provides up to the minute endpoint stats that cover all of the basic sources.</p><ul><li>cpuDescription</li><li>memory</li><li>machineType (Server, Laptop, Desktop)</li><li>machineName</li></ul><p>In addition to the basics, the data source also has some additional data which will prove useful to use later on.</p><ul><li>lastAgentCommunication<ul><li>This tells us when the endpoint was last online. It will be useful in determining system uptime.</li></ul></li><li>agentPublicIP<ul><li>Knowing the country and region where an endpoint resides, helps us calculate carbon intensity. The public IP can be used to identify the geolocation data associated with the agent to put this into practice.</li></ul></li></ul><h4 class="anchor anchorWithStickyNavbar_LWe7" id="collecting-the-data">Collecting the data<a href="#collecting-the-data" class="hash-link" aria-label="Direct link to Collecting the data" title="Direct link to Collecting the data">​</a></h4><p>To make things easier for us, our data source also provides a REST API in which we can collect the data in real time. If your data source doesn’t provide an API, attempt to get regular reports in CSV, XML, or JSON format that you can use in lieu of making API requests.</p><p>Depending on the method of data collection and storage you’ve chosen, some coding is most likely going to be required. It would be extremely beneficial to enlist the help of a developer or data engineer to assist you in reliably collecting and storing this data. Having the ability to retrieve and parse data from an API, work with S3 or a simple database, and create basic automations will come in handy throughout this process. Remember the illustration above.</p><p><img loading="lazy" alt="Workflow Diagram" src="/assets/images/ccf_on_prem_blog_img_1-89a595ff6d485f7f59893e93ad132121.png" width="2000" height="190" class="img_ev3q"></p><p>You’ll want to ensure that whatever method you’ve chosen for data collection, that you’re able to perform the collection at a set timed interval. Tracking machine uptime is paramount to being successful here.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="calculating-uptime-hours">Calculating upTime hours<a href="#calculating-uptime-hours" class="hash-link" aria-label="Direct link to Calculating upTime hours" title="Direct link to Calculating upTime hours">​</a></h4><p>A number of fields in the On-premise Data Model, represent uptime hours of your agent over a period of time. As you collect uptime information about your endpoints, you will want to populate these fields and increment each individual counter.</p><p>Each uptime counter represents a historical period of time. Daily, weekly, yearly, as well as perhaps 30, 60, 90 day increments. These fields may be present in your data source from day one, but perhaps you may need to create and calculate these.</p><p><img loading="lazy" alt="Uptime Diagram" src="/assets/images/ccf_on_prem_blog_img_2-a9e508da4ae357fd9bbec52caa06bf26.png" width="2002" height="398" class="img_ev3q"></p><p>In the diagram above is an oversimplified view of a data collection event for a single endpoint. Let's say for instance the counter you're incrementing is the <strong>dailyUptime</strong> counter. As data about the endpoint comes in it’s determined that the endpoint has been online in the last 1 hour. To increment the daily counter we first need to check the timestamp of when the counter was last reset. If the counter is less than 24 hours old, then we can increment the counter by 1 hour. If it is older than 24 hours we should reset the counter to 1. Additionally you should also reset the timestamp to a current date and time.</p><p>Here is a quick sample of pseudo code to illustrate a couple of these use cases.</p><p><strong>Increment Daily Uptime Counters</strong></p><p><img loading="lazy" alt="Daily Uptime Code Snippet" src="/assets/images/ccf_on_prem_blog_img_5-d94de4a51b869063e2821e011c9c0577.png" width="1354" height="456" class="img_ev3q"></p><p><strong>Increment 30 Day Uptime Counters</strong></p><p><img loading="lazy" alt="Monthly Uptime Code Snippet" src="/assets/images/ccf_on_prem_blog_img_6-9e884cedd79a1c4ca8148c5ad0f90093.png" width="1342" height="470" class="img_ev3q"></p><p>This same principle applies to all other values related to uptime. <strong>weeklyUptime</strong>, <strong>monthlyUptime</strong>, and <strong>yearlyUptime</strong> can all be calculated this way. You can also add additional uptime counters as you see fit; however ensure that the required uptime fields from the On-premise Data Model are present.</p><p>When creating the initial timestamps always remember to start the timestamp from the moment the machine was first added to the data model. It is not advisable to to create an arbitrary initial timestamp as this can cause your uptime fields to be wildly inaccurate.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="conclusion">Conclusion<a href="#conclusion" class="hash-link" aria-label="Direct link to Conclusion" title="Direct link to Conclusion">​</a></h2><p>Collecting on-premise data for activities such as patching, inventory, and lifecycle management has been happening across IT organizations for a very long time. An incredible amount of work has gone into the development and deployment of tooling to achieve those goals. As the world moves more towards implementing green initiatives to better shape their technology futures, being able to calculate and report on the environmental impact of our infrastructures grows as well.</p><p>Even though you may not have a fit for purpose tool to collect on-premise data, that doesn’t mean you can’t still get the data you need. This data most likely already exists in many other tools already in use by your organization. The Anti-Virus used in this post is clearly not meant for this task, but with a bit of trial and error, it can do exactly what is needed to gather all of the necessary data. Be creative and keep an open mind. Data exists everywhere.</p><font size="1">This paper represents an exploratory project undertaken by EA employees to explore ways to leverage existing data and automate methods to calculate electricity use and emissions associated with on-premise endpoints. The statements and opinions expressed in this article are those of the author and do not represent how EA calculates emissions and do not constitute or imply an endorsement of a product, process or service.</font>]]></content:encoded>
            <category>electronicarts</category>
            <category>data</category>
            <category>on-premise</category>
        </item>
        <item>
            <title><![CDATA[Running and Deploying CCF in a Virtual Machine]]></title>
            <link>https://cloud-carbon-footprint.github.io/blog/ccf-on-vm</link>
            <guid>https://cloud-carbon-footprint.github.io/blog/ccf-on-vm</guid>
            <pubDate>Wed, 23 Aug 2023 00:00:00 GMT</pubDate>
            <description><![CDATA[Since the launch of Cloud Carbon Footprint (CCF), our team always aimed to maintain and develop flexibility in both how the tool is used and the options for its deployment. With such a large variety of infrastructure across organizations, we strive to balance supporting as many environments out of the box as possible while also preserving the customization options you may need.]]></description>
            <content:encoded><![CDATA[<p>Since the launch of Cloud Carbon Footprint (CCF), our team always aimed to maintain and develop flexibility in both how the tool is used and the options for its deployment. With such a large variety of infrastructure across organizations, we strive to balance supporting as many environments out of the box as possible while also preserving the customization options you may need.</p><p>One of the most popular options for both piloting a CCF instance as a proof of concept as well as for an internal production environment is to run CCF within a virtual machine. After all, it is a great way to get your feet wet with CCF without bringing extra dependencies on your local machine, tying up any resources, and enabling options for an always-on service that expands beyond your local machine. Alternatively, you may want to get creative by customizing or automating how CCF fits into your use cases. For example, perhaps you may want to set up a cloud function that automatically fetches cloud estimates for every recent day, week, or month? Or maybe you want a fully accessible CCF API for members of your organization or team to query? If any of these benefits sound appealing then the VM option might be for you, and you’re in the right place!</p><p>Virtual machines come in many shapes and sizes across different cloud provider platforms, including AWS EC2, Google Cloud Compute Engine, and Azure Virtual Machine services. For the sake of this article, we’ll be focusing on how to create a CCF application running on an AWS EC2 instance. However, depending on the chosen operating system or distribution of your machine, the steps should be relatively the same!</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="creating-your-ec2-instance">Creating your EC2 Instance<a href="#creating-your-ec2-instance" class="hash-link" aria-label="Direct link to Creating your EC2 Instance" title="Direct link to Creating your EC2 Instance">​</a></h2><p>CCF at its core is a Node application running Express.JS for its API and React for its client. So once you have the environment setup for one endpoint of the application, you’ll be able to run instances of CCF’s API, CLI, and Client on the same machine, and switch between them if desired. Before we do that, let’s create our machine and set up our environment.</p><p>To start, we assume that you already have the following:</p><ul><li>An existing AWS account with permissions for creating and managing EC2 instances</li><li>Followed the steps for setting up billing data for your <a href="https://www.cloudcarbonfootprint.org/docs/aws" target="_blank" rel="noopener noreferrer">cloud provider</a> (i.e. AWS steps 1-3)</li><li>Basic familiarity with navigating the AWS console</li></ul><p>Let’s navigate to the EC2 dashboard and select the option to launch a new instance using the shiny “Launch Instances” button:</p><p><img loading="lazy" alt="Launch Instances Button" src="/assets/images/ccf_vm_blog_img_1-7fc64f56cfe5140c428efc9a8eda2b4b.png" width="1319" height="68" class="img_ev3q"></p><p>From this point, we’ll be selecting the configuration options for our new machine. While the free tier may be tempting, we’ll go with a t2.medium. I’ve found that on smaller instances such as a t2.micro, the limited hardware can sometimes cause issues when installing node modules or running the app. So we could use the extra “oomph”. However, for the sake of your own instance, please consider the following:</p><ul><li>If you’re expecting a large amount of estimates, consider a larger instance with higher memory and compute power.</li><li>If you plan on running a <a href="https://www.cloudcarbonfootprint.org/docs/data-persistence-and-caching#mongodb-storage" target="_blank" rel="noopener noreferrer">MongoDB instance</a> on the same machine and persisting a large amount of estimates, consider increasing the storage capacity of your instance.</li><li>If costs and efficiency is top of mind, consider choosing the option for an ARM instance as you will not be able to migrate from a non-ARM instance afterwards.</li><li>Running EC2 instances incur costs! So make sure to stop/delete instances when done with them and that you choose a capable instance that fits within your budget.</li></ul><p><img loading="lazy" alt="Launch Instances Config" src="/assets/images/ccf_vm_blog_img_2-9e297cb0fc5f23365fc19ccb50817536.png" width="863" height="636" class="img_ev3q"></p><p>You may also notice that we’ll be going with an Ubuntu image as our operating system – a popular and widely accepted distribution that you can use with any VM host. You’re welcome to choose a different Linux-based operating system such as Amazon Linux in the case of an EC2 instance. Amazon Linux serves as a Linux distribution optimized for running in AWS. It is also optimized for running most Linux-based software making the steps you’ll follow almost identical. For the sake of keeping this tutorial a little more friendly for other cloud VM services, we’ll be sticking with Ubuntu.</p><p>After making some final decisions in creating a key pair (required for connecting via an SSH client) and choosing a security group, we’re going to hit the even more shiny “Launch Instance” button. After a short wait, you should see a notification that the instance has been created and is running. So let’s connect to it!</p><p><em>Side Note</em>: If you’re more comfortable with the <a href="https://aws.amazon.com/cli/" target="_blank" rel="noopener noreferrer">cloud provider CLI</a> or other ways of configuring resources, these steps can be replicated using those methods as well.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="setting-up-your-ccf-instance">Setting Up Your CCF Instance<a href="#setting-up-your-ccf-instance" class="hash-link" aria-label="Direct link to Setting Up Your CCF Instance" title="Direct link to Setting Up Your CCF Instance">​</a></h2><p>Connect to your instance using your preferred method – whether it be in the cloud provider console or through a local terminal via SSH. Once you’re in, we will need to configure NPM and Node so that our server can support CCF’s code.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="installing-nodejs">Installing Node.JS<a href="#installing-nodejs" class="hash-link" aria-label="Direct link to Installing Node.JS" title="Direct link to Installing Node.JS">​</a></h3><p>We’ll be following the officially recommended steps for <a href="https://docs.aws.amazon.com/sdk-for-javascript/v2/developer-guide/setting-up-node-on-ec2-instance.html" target="_blank" rel="noopener noreferrer">setting up Node.JS on an EC2 Instance</a>. Henceforth, we will be relying on NVM to make this an easy process! <a href="https://www.bing.com/search?pglt=641&amp;q=nvm&amp;cvid=ef35fe5448b345eba7740e8aae9b0b8e&amp;aqs=edge..69i57j0l5j69i61l3.383j0j1&amp;FORM=ANNTA1&amp;PC=U531" target="_blank" rel="noopener noreferrer">NVM</a> will manage our versions of Node for us, making migrating or downgrading easier.</p><div class="language-console codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#bfc7d5;--prism-background-color:#292d3e"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-console codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#bfc7d5"><span class="token plain">curl -o- https://raw.githubusercontent.com/nvm-sh/nvm/v0.39.5/install.sh | bash</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><p>We then need to activate NVM by running the following command:</p><div class="language-console codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#bfc7d5;--prism-background-color:#292d3e"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-console codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#bfc7d5"><span class="token plain">. ~/.nvm/nvm.sh</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><p>You can verify that it is active by running <code>nvm -v</code> in which you should see an output of <code>0.39.5</code> or similar.</p><p>CCF requires NodeJS 16 or later. While I prefer 18 since it is the current LTS release, you will need to run the following command – replacing the number with the version that you wish to install:</p><div class="language-console codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#bfc7d5;--prism-background-color:#292d3e"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-console codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#bfc7d5"><span class="token plain">nvm install 18</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><p><em>Note</em>: Make sure to check the latest version compatibility that your image supports. For example, Amazon Linux 2 does not support Node 18 at the time of writing this article.</p><p>NVM will also inform you that it is setting the current version as the default. This is useful for whenever you need to switch node versions or have node disabled for some reason, as you can use <code>nvm use –default</code> to re-enable it. You can verify that Node is successfully installed by running <code>node -v</code>, in which you should see an output of the version that you installed.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="installing-yarn">Installing Yarn<a href="#installing-yarn" class="hash-link" aria-label="Direct link to Installing Yarn" title="Direct link to Installing Yarn">​</a></h3><p>While installing node also automatically installs npm, CCF uses <a href="https://v3.yarnpkg.com/" target="_blank" rel="noopener noreferrer">Yarn</a> as its package manager. Fortunately, Node 16 and up make it super easy to install thanks to its inclusion of <a href="https://github.com/nodejs/corepack" target="_blank" rel="noopener noreferrer">corepack</a>.</p><p>To install Yarn, we simply need to enable corepack using the following command:</p><div class="language-console codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#bfc7d5;--prism-background-color:#292d3e"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-console codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#bfc7d5"><span class="token plain">corepack enable</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><p>With a little magic, you can now use <code>yarn -v</code> to verify that yarn is now installed and enabled in your server. You’ll see that version 1 (yarn classic) is enabled. If you’ve been reading the CCF documentation, you may notice that it requires Yarn 3. Do not fear, CCF will take care of the upgrade during its installation.</p><p>Now for the fun part!</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="installing-ccf">Installing CCF<a href="#installing-ccf" class="hash-link" aria-label="Direct link to Installing CCF" title="Direct link to Installing CCF">​</a></h3><p>There are multiple ways to install CCF, as noted in the <a href="https://www.cloudcarbonfootprint.org/docs/getting-started" target="_blank" rel="noopener noreferrer">Getting Started</a> section. While you can clone the app and get full access to all of its packages and latest features as soon as they are available, we’re going to go with the <a href="https://www.cloudcarbonfootprint.org/docs/create-app" target="_blank" rel="noopener noreferrer">Create App</a> option as the more stable and simple solution.</p><p>To get started, we will run the create-app command with an additional flag to note that we wish to skip the <code>yarn install</code> step. This gives us flexibility in case we wish to connect our data using the <a href="https://www.cloudcarbonfootprint.org/docs/getting-started#guided-install" target="_blank" rel="noopener noreferrer">guided install</a> process, which will end up doing the installation step for us.:</p><div class="language-console codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#bfc7d5;--prism-background-color:#292d3e"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-console codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#bfc7d5"><span class="token plain">npx @cloud-carbon-footprint/create-app --skip-install</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><p>You’ll be asked to install the latest version of the <code>create-app</code> package, in which you can reply “yes”.</p><p><img loading="lazy" alt="Name Create App" src="/assets/images/ccf_vm_blog_img_3-83fab40b1161b84dd7c91f076f706d56.png" width="715" height="123" class="img_ev3q"></p><p>When prompted for a name, feel free to choose whichever name you prefer your app to have. In this example we’ll be going <code>my-ccf-app</code>. Take note that this will also be the name of the directory that the app will be created in. So you may want to make sure the name is unique and matches any folder naming conventions you may have.</p><p>Afterwards, you’ll see that the script takes care of creating your files and moving them to a directory named after your app. Your app will be successfully created!</p><p><img loading="lazy" alt="Create App Successful" src="/assets/images/ccf_vm_blog_img_4-37e060049caf9fbb23067239b8191aaa.png" width="722" height="117" class="img_ev3q"></p><p>Use <code>cd my-ccf-app</code> to switch to the directory of your app. You should see the following contents within your directory:</p><div class="language-console codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#bfc7d5;--prism-background-color:#292d3e"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-console codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#bfc7d5"><span class="token plain">lerna.json  package.json  packages  tsconfig.json</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><p><em>Note</em>: If you run <code>yarn -v</code> again while in the directory, you’ll see that the version has automatically updated to 3.1.1 like magic. ✨</p><p>At this moment, you can either connect your data by manually creating a <code>.env</code> file in either your <code>packages/api</code> or <code>packages/cli</code> directory based on the <code>.env.template</code> files in those same directories. Make sure to check out the documentation on how to connect data for your chosen cloud provider:</p><ul><li><a href="https://www.cloudcarbonfootprint.org/docs/aws" target="_blank" rel="noopener noreferrer">AWS</a></li><li><a href="https://www.cloudcarbonfootprint.org/docs/gcp" target="_blank" rel="noopener noreferrer">Google Cloud</a></li><li><a href="https://www.cloudcarbonfootprint.org/docs/azure" target="_blank" rel="noopener noreferrer">Azure</a></li></ul><p>Alternatively, you can use the guided installation method mentioned above to run a friendly CLI program that will walk you through setting up your credentials and will create the <code>.env</code> files for you!</p><p>If you’d rather skip connecting your data altogether, you can also <a href="https://www.cloudcarbonfootprint.org/docs/run-with-mocked-data" target="_blank" rel="noopener noreferrer">run with mock data</a> instead and move on to the next step.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="running-your-app">Running Your App<a href="#running-your-app" class="hash-link" aria-label="Direct link to Running Your App" title="Direct link to Running Your App">​</a></h2><p>After following the setup method of your choice, let’s top things off by doing a <code>yarn install</code>. Once the dependencies are done installing, we’re good to <a href="https://www.cloudcarbonfootprint.org/docs/getting-started#starting-the-app" target="_blank" rel="noopener noreferrer">start our app</a>!</p><p>You can now use <code>yarn start</code> to concurrently start both the Client (react dashboard) and the API (express app).</p><ul><li>If running the client or with mock data, your CCF Dashboard will be available at port 3000 of your instance. You can view the dashboard by navigating to the public IP of your instance followed by the corresponding port.<ul><li>Please note, you will need to configure your security or network settings to make this port available</li></ul></li><li>You can also use <code>yarn start-api</code> instead to only <a href="https://www.cloudcarbonfootprint.org/docs/running-the-api" target="_blank" rel="noopener noreferrer">run the API</a>. You can verify that the API is running by making a request to one of the endpoints on port 4000 of the instance.<ul><li>For example, try making the following command in another terminal instance:<br>`curl http://[your-ip]:4000/api/regions/emissions-factors`</li><li>Please note, you will need to configure your security or network settings to make this port available if attempting to make requests outside of the instance.</li></ul></li><li>You can also use yarn <code>start-cli</code> for <a href="https://www.cloudcarbonfootprint.org/docs/running-the-cli" target="_blank" rel="noopener noreferrer">running the CLI</a> and requesting estimates directly within the terminal.</li></ul><p>Congratulations! You now have successfully created a CCF app running in a virtual machine.</p><p>You may notice that if you exit the SSH or terminal session, that the running process will not persist. In this case, you can use a tool such as a <a href="https://www.gnu.org/software/screen/" target="_blank" rel="noopener noreferrer">Screen</a> to create an uninterruptible terminal session to run your app in. To do so, try running the following command:</p><div class="language-console codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#bfc7d5;--prism-background-color:#292d3e"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-console codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#bfc7d5"><span class="token plain">screen -S ccf</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><p>This will create a new Screen session called “ccf”. From here, you can run one of the <code>yarn start</code> commands to run your app and then use <code>ctrl+a</code> <code>ctrl+d</code> command keys to detach from the session. The session will stay on in the background and your CCF app will stay running!</p><p>You can always reattach to the session by entering <code>screen -r</code> in your terminal.</p><p>If you’re more comfortable and don’t like the idea of having terminal sessions running in the background, you can also create a <code>.service</code> file to <a href="https://stackoverflow.com/questions/4018154/how-do-i-run-a-node-js-app-as-a-background-service" target="_blank" rel="noopener noreferrer">run your application as a background service</a> instead.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="what-now">What Now?<a href="#what-now" class="hash-link" aria-label="Direct link to What Now?" title="Direct link to What Now?">​</a></h2><p>Now that you have an always-running CCF instance on a cloud-based virtual machine, you can now continue to explore both realtime and historical estimates for all of your services in the cloud. If you’d like to explore additional ways to enhance your CCF app, consider the following:</p><ul><li><a href="https://www.cloudcarbonfootprint.org/docs/run-with-docker" target="_blank" rel="noopener noreferrer">Running the CCF App with Docker</a></li><li>Creating a cron-triggered cloud function to automatically fetch new estimates</li><li>Configuring a MongoDB instance to <a href="https://www.cloudcarbonfootprint.org/docs/data-persistence-and-caching#mongodb-storage" target="_blank" rel="noopener noreferrer">persist new and historical estimate data</a></li><li>Using the CLI app to <a href="https://www.cloudcarbonfootprint.org/docs/data-persistence-and-caching#seeding-cache-file" target="_blank" rel="noopener noreferrer">seed data into the configured cache option</a> for your instance</li><li>Creating an internal dashboard for your team or organization to view estimate data</li></ul><p>Hopefully you’ve found this walkthrough helpful and see that this is only the beginning of your cloud carbon footprint journey and taking steps to help create a greener cloud! For more walkthroughs and technical deep dives, make sure to keep following the <a href="http://cloudcarbonfootprint.org/blog" target="_blank" rel="noopener noreferrer">CCF Blog</a> and share your experience on our <a href="https://github.com/cloud-carbon-footprint/cloud-carbon-footprint/discussions" target="_blank" rel="noopener noreferrer">discussions page</a>!</p>]]></content:encoded>
            <author>arik.m.smith@gmail.com (Arik Smith)</author>
            <category>thoughtworks</category>
            <category>deployment</category>
        </item>
    </channel>
</rss>